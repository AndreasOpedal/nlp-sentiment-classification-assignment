{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "# nltk.download('opinion_lexicon')\n",
    "import nltk\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "SEED = 76\n",
    "random.seed(76)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task a) \n",
    "\n",
    "Download the Pang and Lee movie review data: http://www.cs.cornell.edu/people/pabo/movie-review-data/ \n",
    "\n",
    "\n",
    "Hold out a randomly-selected 400 reviews as a test set. Download a sentiment lexicon, such as the one currently available from Bing Liu: https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "pos_path = 'txt_sentoken/pos/'\n",
    "neg_path = 'txt_sentoken/neg/'\n",
    "pos_names = listdir(pos_path)\n",
    "neg_names = listdir(neg_path)\n",
    "\n",
    "pos_texts = []\n",
    "for i in range(0, len(pos_names)):\n",
    "    text = open(pos_path + pos_names[i], \"r\").read()\n",
    "    pos_texts.append(text)\n",
    "neg_texts = []\n",
    "for j in range(0, len(neg_names)):\n",
    "    text = open(neg_path + neg_names[i], \"r\").read()\n",
    "    neg_texts.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '1' '0' '0' '1' '0' '0' '0' '0' '0']\n"
     ]
    }
   ],
   "source": [
    "# Combine the data with labels in numpy array\n",
    "# Label positive as 1 and negative as 0\n",
    "pos_label = 1\n",
    "neg_label = 0\n",
    "\n",
    "data = np.append(np.array([pos_texts, np.full((len(pos_texts)), pos_label)]).T,\n",
    "          np.array([neg_texts, np.full((len(neg_texts)), neg_label)]).T, axis = 0)\n",
    "\n",
    "# shuffle data\n",
    "\n",
    "random.shuffle(data)\n",
    "print(data[:,1][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get sentiment lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_set = set(opinion_lexicon.positive())\n",
    "neg_set = set(opinion_lexicon.negative())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i)\n",
    "\n",
    "Tokenize the data using a library of your choosing and classify each document as\n",
    "positive if it has more positive sentiment words than negative sentiment words.\n",
    "Compute the accuracy and F-measure on detecting positive reviews on the test\n",
    "set, using this lexicon-based classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data by a regex tokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+,')\n",
    "tokenizer.tokenize(data[0,0])\n",
    "\n",
    "tokenized = []\n",
    "for text in data:\n",
    "    processed = tokenizer.tokenize(text[0])\n",
    "    tokenized.append([processed, text[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600,) (400,) (1600,) (400,)\n"
     ]
    }
   ],
   "source": [
    "# Split in train and test set\n",
    "\n",
    "X = np.array(tokenized)[:,0]\n",
    "y = np.array(tokenized)[:,1].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = SEED)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a lexicon-based classifier\n",
    "\n",
    "def classify_by_lexicon(text, pos_lexicon, neg_lexicon, verbose = False):\n",
    "    '''Takes a text as a list of tokenized strings and classifies binary sentiment\n",
    "        based on counting words occuring in the positive and negative lexicons'''\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for word in text:\n",
    "        if word in pos_lexicon:\n",
    "            pos_count+=1\n",
    "        elif word in neg_lexicon:\n",
    "            neg_count+=1\n",
    "    if verbose:\n",
    "        print(\"Positive fraction: \", pos_count / (pos_count+neg_count))\n",
    "    if pos_count >= neg_count:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive fraction:  0.3392857142857143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on examples\n",
    "i = 4\n",
    "classify_by_lexicon(X_train[i], pos_set, neg_set, verbose = True), y_train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and compute accuracy & F1-score on test set\n",
    "\n",
    "y_pred1 = [classify_by_lexicon(X_test[i], pos_set, neg_set) for i in range(0, len(y_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of simple lexicon-based classifier on test set:  0.85\n",
      "F1-score of simple lexicon-based classifier on test set:  0.8170731707317073\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, y_pred1)\n",
    "f1 = f1_score(y_test, y_pred1)\n",
    "print(\"Accuracy of simple lexicon-based classifier on test set: \", acc)\n",
    "print(\"F1-score of simple lexicon-based classifier on test set: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii)\n",
    "\n",
    "Then train a discriminative classifier (averaged perceptron or logistic regression)\n",
    "on the training set, and compute its accuracy and F-measure on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction\n",
    "\n",
    "Q: Does a logistic regression classifier with features for number of positive words and number of negative words do better?\n",
    "\n",
    "Hypothesis: Yes, optimal decision boundary might be other than 0.5 fraction of positive words. A logistic regression classifier is more flexible and could learn a different decision boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text, pos_lexicon, neg_lexicon):\n",
    "    '''Returns number of positive and negative words in text, according to lexicons'''\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for word in text:\n",
    "        if word in pos_lexicon:\n",
    "            pos_count+=1\n",
    "        elif word in neg_lexicon:\n",
    "            neg_count+=1\n",
    "    return pos_count, neg_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from train and test data\n",
    "\n",
    "X_train_processed = []\n",
    "for x in X_train:\n",
    "    pos, neg = extract_features(x, pos_set, neg_set)\n",
    "    X_train_processed.append([pos,neg])\n",
    "X_train_processed = np.array(X_train_processed)\n",
    "\n",
    "X_test_processed = []\n",
    "for x in X_test:\n",
    "    pos, neg = extract_features(x, pos_set, neg_set)\n",
    "    X_test_processed.append([pos,neg])\n",
    "X_test_processed = np.array(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set:  0.9625\n",
      "F1-score of logistic regression classifier on test set:  0.9597855227882037\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression with default hyperparameters and evaluate on test set\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_processed, y_train)\n",
    "y_pred2 = clf.predict(X_test_processed)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred2)\n",
    "f1 = f1_score(y_test, y_pred2)\n",
    "print(\"Accuracy of logistic regression classifier on test set: \", acc)\n",
    "print(\"F1-score of logistic regression classifier on test set: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Yes, so seems to be the case! Let's test this hypothesis with a permutation test in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii)\n",
    "\n",
    "Qualitatively compare the difference in accuracy between the two methods. Does\n",
    "the discriminative classifier do overwhelmingly better than guessing the mode?\n",
    "Describe a method for quantifying the significance of the difference between the\n",
    "two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lexicon-based classifier achieves an accuracy of 0.86 while the logistic classifier achieves an accuracy of 0.97, so it does indeed seem to do significantly better. Can take a look at confusion matrix of both classifiers to see where the lexicon-based classifier fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon: \n",
      " [[206   0]\n",
      " [ 60 134]]\n",
      "Logistic: \n",
      " [[206   0]\n",
      " [ 15 179]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Lexicon: \\n\", confusion_matrix(y_test, y_pred1))\n",
    "print(\"Logistic: \\n\", confusion_matrix(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the confusion matrix of the lexicon-based classifier, all its mistakes come from predicting positive sentiment when the true sentiment is negative. Which means that the threshold for fraction of positive words can probably be increased to get a higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired permutation test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null hypothesis: The accuracy of the logistic regression classifier is the same as the accuracy of the lexicon-based model\n",
    "#### Alternative hypothesis: The accuracy of the logistic regression classifier is higher than the accuracy of the lexicon-based model\n",
    "\n",
    "Procedure:\n",
    "\n",
    "* Split training data into 10 splits\n",
    "* Train both models on each of the splits and record the accuracy on test set\n",
    "* Compute the difference of the average of the accuracies of both methods, taking this to be our observed statistic\n",
    "* Permute the orders of the accuracies and compute the diffference in average for each of the permutations\n",
    "* Compare the observed statistic to the distribution of the permuted statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process X\n",
    "X_processed = []\n",
    "for x in X:\n",
    "    pos, neg = extract_features(x, pos_set, neg_set)\n",
    "    X_processed.append([pos,neg])\n",
    "X_processed = np.array(X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 10\n",
    "# Note that with 10 splits, we have 10! possible permutations which is a lot. Sample only a subset of these\n",
    "num_permutations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracies(X_train2, y_train, X_test1, X_test2, y_test, pos_lexicon, neg_lexicon):\n",
    "    \n",
    "    # Accuracy of lexicon-based classifier\n",
    "    y_pred1 = [classify_by_lexicon(X_test1[i], pos_set, neg_set) for i in range(0, len(y_test))]\n",
    "    acc1 = accuracy_score(y_test, y_pred1)\n",
    "    \n",
    "    # Accuracy of logistic regression classifier\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train2, y_train)\n",
    "    y_pred2 = clf.predict(X_test2)\n",
    "    acc2 = accuracy_score(y_test, y_pred2)\n",
    "    \n",
    "    return acc1, acc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the accuracy scores for the num_splits splits\n",
    "\n",
    "diffs = []\n",
    "accs = []\n",
    "X_test = X[1600:]\n",
    "X_test_processed = X_processed[1600:]\n",
    "y_test = y[1600:]\n",
    "for i in range(0, num_splits):\n",
    "    X_train = X[160*i:160*(i+1)]\n",
    "    X_train_processed = X_processed[160*i:160*(i+1)]\n",
    "    y_train = y[160*i:160*(i+1)]\n",
    "    acc1, acc2 = compute_accuracies(X_train_processed, y_train, X_test, X_test_processed, y_test, pos_set, neg_set)\n",
    "    diffs.append(acc2-acc1)\n",
    "    accs.append([acc1,acc2])\n",
    "accs = np.array(accs)\n",
    "obs = np.mean(diffs)\n",
    "\n",
    "def mean(array):\n",
    "    return np.mean(array[:,1] - array[:,0])\n",
    "\n",
    "def shuffle_along_axis(a, axis):\n",
    "    idx = np.random.rand(*a.shape).argsort(axis=axis)\n",
    "    return np.take_along_axis(a,idx,axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "for i in range(0, num_permutations):\n",
    "    arr = shuffle_along_axis(accs, 1)\n",
    "    means.append(mean(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(means, density = True)\n",
    "plt.vlines(obs, ymin = 0, ymax = 40, color = \"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly visible in the histogram above that the observed mean difference is significant. But let's compute the p-value anyway (this is a one-sided test since our alternative hypothesis was that the difference is larger than zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test gives a p-value of:  0.0013998600139986002 \n",
      "We thus reject our null hypothesis\n"
     ]
    }
   ],
   "source": [
    "p_value = (sum(np.array(means)>=obs) + 1)/(len(means)+1)\n",
    "print(\"The test gives a p-value of: \", p_value, \"\\nWe thus reject our null hypothesis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
